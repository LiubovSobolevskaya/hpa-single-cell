#!/bin/bash

fold_i=$1

if [ "$#" -eq 1 ]; then
  gpu_id=$1
else
  gpu_id=$2
fi

gpu_id=$(($gpu_id%2))
# aug and grad accum after the 1st epoch


python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 4 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_4e5  --load-state-dict-path "../output/models/densenet121_1024_all_data__obvious_neg__gradaccum_20__start_lr_3e6/fold${fold_i}/final.pth" --loss FocalSymmetricLovaszHardLogLoss > "bestfitting_cellwise_512_densenet_fold${fold_i}.log"


if [ "$fold_i" -eq 1 ]; then
  python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 6 --scheduler-lr-multiplier 1 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_1e5  --load-state-dict-path "../output/models/densenet121_512_cellwise__gradaccum_4__start_lr_4e5/fold${fold_i}/final.pth" --loss FocalSymmetricLovaszHardLogLoss --eval-at-start --target-raw-img-size 1024 --include-nn-mitotic > "bestfitting_cellwise_512_more_mitotoic_densenet_fold${fold_i}_target_img_1024.log"
  python -m src.train.train_cellwise --scheduler-epoch-offset 5 --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 1 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_1e5_focal  --load-state-dict-path "../output/models/densenet121_512_cellwise__gradaccum_4__start_lr_1e5/fold${fold_i}/final.pth" --loss FocalSymmetricHardLogLoss --eval-at-start --target-raw-img-size 1024 --include-nn-mitotic > "bestfitting_cellwise_512_more_mitotoic_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss.log"
  python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 1 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 1 --scheduler-lr-multiplier 0.07 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_7e7_focal_no_negs_upsampled_minorities  --load-state-dict-path "../output/models/densenet121_512_cellwise__gradaccum_4__start_lr_1e5_focal/fold${fold_i}/final.pth" --loss FocalSymmetricHardLogLoss --eval-at-start --target-raw-img-size 1024 --include-nn-mitotic --ignore-negative --upsample-minorities > "bestfitting_cellwise_512_more_mitotoic_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss_no_negs_upsampled_minorities.log"
else
  python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 1 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_1e5_focal  --load-state-dict-path "../output/models/densenet121_512_cellwise__gradaccum_4__start_lr_4e5/fold${fold_i}/final.pth" --loss FocalSymmetricHardLogLoss --eval-at-start --target-raw-img-size 1024 --include-nn-mitotic > "bestfitting_cellwise_512_more_mitotoic_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss.log"
  python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 1 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 1 --scheduler-lr-multiplier 0.07 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_7e7_focal_no_negs_upsampled_minorities  --load-state-dict-path "../output/models/densenet121_512_cellwise__gradaccum_4__start_lr_1e5_focal/fold${fold_i}/final.pth" --loss FocalSymmetricHardLogLoss --eval-at-start --target-raw-img-size 1024 --include-nn-mitotic --ignore-negative --upsample-minorities > "bestfitting_cellwise_512_more_mitotoic_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss_no_negs_upsampled_minorities.log"

fi

if [ "$fold_i" -eq 0 ]; then
  python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --img_size 512 --batch_size 32 --workers 10 --gradient-accumulation-steps 1 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 2 --scheduler-lr-multiplier 0.06 --out_dir densenet121_512_cellwise__gradaccum_4__start_lr_7e7_focal_no_negs_upsampled_minorities  --resume final.pth --loss FocalSymmetricHardLogLoss --eval-at-start --target-raw-img-size 1024 --include-nn-mitotic --ignore-negative --upsample-minorities > "bestfitting_cellwise_512_more_mitotoic_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss_no_negs_upsampled_minorities.log"

fi

fold_i=0
python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --arch class_efficientnet_dropout --effnet-encoder efficientnet-b4 --img_size 512 --batch_size 8 --workers 22 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 3 --scheduler-lr-multiplier 30 --out_dir effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_3e4  --normalize --loss FocalSymmetricLovaszHardLogLoss > "effnetb4_cellwise_512_densenet_fold${fold_i}.log"
python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --arch class_efficientnet_dropout --effnet-encoder efficientnet-b4 --img_size 512 --batch_size 8 --workers 10 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 1 --scheduler-lr-multiplier 1 --out_dir effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_1e5_focal  --load-state-dict-path "../output/models/effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_3e4/fold${fold_i}/final.pth" --loss FocalSymmetricHardLogLoss --target-raw-img-size 1024 --include-nn-mitotic > "effnetb4_cellwise_512_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss.log"
python -m src.train.train_cellwise --fold $fold_i --all-gpus --arch class_efficientnet_dropout --effnet-encoder efficientnet-b4 --img_size 512 --batch_size 16 --workers 22 --gradient-accumulation-steps 1 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 1 --scheduler-lr-multiplier 0.1 --out_dir effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_1e6_focal_no_negs_upsampled_minorities  --load-state-dict-path "../output/models/effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_1e5_focal/fold${fold_i}/final.pth" --loss FocalSymmetricHardLogLoss --target-raw-img-size 1024 --include-nn-mitotic --ignore-negative --upsample-minorities > "effnetb4_cellwise_512_densenet_fold${fold_i}_target_img_1024_FocalSymmetricHardLogLoss_no_negs_upsampled_minorities.log"


#python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --arch class_efficientnet_dropout --effnet-encoder efficientnet-b4 --img_size 512 --batch_size 8 --workers 22 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 30 --out_dir effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_3e4  --normalize --loss FocalSymmetricLovaszHardLogLoss > "effnetb8_cellwise_512_densenet_fold${fold_i}.log"
#
#
#python -m src.train.train_cellwise --fold $fold_i --gpu-id $gpu_id --target-raw-img-size 1024 --arch class_efficientnet_dropout --effnet-encoder efficientnet-b4 --img_size 512 --batch_size 8 --workers 22 --gradient-accumulation-steps 4 --cell-level-labels-path ../output/densenet121_pred.h5 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 30 --out_dir effnetb4_512_cellwise__batch_8_gradaccum_4__start_lr_3e4  --normalize --loss FocalSymmetricLovaszHardLogLoss --resume final.pth > "effnetb8_cellwise_512_densenet_fold${fold_i}.log"


#nohup python -m src.train.train_cellwise --img_size 512 --batch_size 32 --workers 20 --scheduler Adam10WarmUp --epochs 10 --out_dir "img_level_cellwise_densenet121_512_all_data__obvious_neg__adam10warmup"  --load-state-dict-path "use-img-level-densenet-ckpt" --gradient-accumulation-steps 50 --fold $fold_i --gpu-id $gpu_id --image-level-labels > img_level_cellwise_fold_0.log &#######
#
#nohup python -m src.train.train_cellwise --img_size 512 --batch_size 8 --workers 20 --scheduler Adam10WarmUp --epochs 10 --out_dir "img_level_cellwise_effnetb7_512_all_data__obvious_neg__adam10warmup" --gradient-accumulation-steps 200 --fold 0 --image-level-labels --normalize --arch class_efficientnet_dropout --effnet-encoder efficientnet-b4 > img_level_cellwise_fold_0_effnetb4.log &
#python -m src.train.train_bestfitting --fold 1 --gpu-id 1 --img_size 1024 --batch_size 8 --workers 22 --gradient-accumulation-steps 400 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 0.05 --out_dir densenet121_1024_all_data__obvious_neg__gradaccum_20__start_lr_3e6  --load-state-dict-path "../output/models/densenet121_1024_all_data__obvious_neg__gradaccum_200__start_lr_4e5/fold1/010.pth" --eval-at-start > "bestfitting_finetuning_1024_densenet_fold1_final.log"
#python -m src.train.train_bestfitting --fold 1 --gpu-id 1 --img_size 1024 --batch_size 8 --workers 22 --gradient-accumulation-steps 400 --scheduler Adam10 --epochs 10 --scheduler-lr-multiplier 0.05 --out_dir densenet121_1024_all_data__obvious_neg__gradaccum_20__start_lr_3e6  --load-state-dict-path "../output/models/densenet121_1024_all_data__obvious_neg__gradaccum_200__start_lr_4e5/fold1/010.pth" --eval-at-start > "bestfitting_finetuning_1024_densenet_fold1_final.log"